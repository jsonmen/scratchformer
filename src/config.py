DATASET_DOWNLOAD_LINK = "https://www.kaggle.com/api/v1/datasets/download/mohamedlotfy50/wmt-2014-english-german"
DATASET_TRAIN_FILE = "data/wmt14_translate_de-en_train.csv"
DATASET_TEST_FILE = "data/wmt14_translate_de-en_test.csv"
DATASET_VAL_FILE = "data/wmt14_translate_de-en_validation.csv"
TENSORBOARD_OUTPUT_DIR = "runs/try3"
CHECKPOINT_PATH = "checkpoints/try3"
TEST_TRANSLATION_TEXT = """Cross-text connections questions give us twice as much text to consider, but both texts will cover the same subject, and this close interrelation means that each text will build your understanding of the other."""
VOCAB_SIZE = 37000
TOKENIZER_PREFIX = "model/tokenizer"
TOKENIZER_PATH = TOKENIZER_PREFIX + ".model"
TOKENIZER_OPTIONS = dict(
    input_format="text",
    model_prefix=TOKENIZER_PREFIX,
    model_type="bpe",
    vocab_size=VOCAB_SIZE,
    byte_fallback=True,
    normalization_rule_name="identity",
    allow_whitespace_only_pieces=True,
    add_dummy_prefix=True,
    split_digits=True,
    split_by_unicode_script=True,
    split_by_number=True,
    split_by_whitespace=True,
    max_sentencepiece_length=16,
    pad_id=0,
    unk_id=1,
    bos_id=2,
    eos_id=3,
    num_threads=8,
    minloglevel=2,
)
BATCH_SIZE = 32
MAX_SEQ_LEN = 512
NUM_WORKERS = 1
PIN_MEMORY = False
N_ENCODER_LAYERS = N_DECODER_LAYERS = 6
D_MODEL = 256
D_FF = D_MODEL * 4
N_HEADS = 4
DROPOUT = 0.1
LABEL_SMOOTHING = 0.1
WARMUP_STEPS = 4000
CHECKPOINT_STEPS = 10000
EPOCH = 50
LOAD_CHECKPOINT = True
SUBSET_SIZE = 1_500_000
LR_SCALE = 1.2
WEIGHT_DECAY = 1e-3
GRAD_CLIP = 0.5
FP16 = False
ACCUM_STEPS = 4
